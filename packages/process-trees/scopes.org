#+TITLE:scopes

* the idea

*UPDATE*: this is about (sub)systems.  Which among other things is a scope.
Particularly, a subsystem provides a way for (contingent) processes to name
things freely while still comprising a larger system where each thing has a
unique, non-conflicting name.

In yet other words, a subsystem is a live namespace.

Can something be a live namespace without being a subsystem?  If it is not a
creator of processes.  Are there such things?  It depends on what you mean by
live namespace.  Something like the multiwave (which is a kind of port resolver)
does create (or at least provide) subscribables corresponding to a set of names,
and minting them (for later binding) if necessary.

A knowledge-based runtime will require a notion of "scopes" even for the
simplest cases.

What is a scope?

In RDF terms, you can think of a scope as a named graph with a special
entailment rule applied.  This entailment rule lets you state that graph A
entails graph B.  The effect of this is that all facts that are true in B are
true in A.

But in not-RDF terms, you can think of a scope as a namespace.  What
distinguishes the things in a scope is that they have a common /owner/, and by
extension a common provenance.

Note that if all sources are monotonic (increasing only), then constituents can
be comingled without the need for scopes (though note that there may be no way
to track the provenance of facts).  If a source may /stop/ contributing a
particular item, scopes provide a way to isolate each set of contributions so
that multiple contributors can contribute the same fact independently.

An outcome of scopes /should/ be that there are certain things which can never
collide, but note that this contradicts the previous point (that multiple scopes
can refer to "the same" fact).

Scopes become useful only if they partition a system into exclusive subsystems
(in a way that is non-overlapping), where the boundaries represent lines of
communication.

* entailment

A subsystem /entails/ its constituents.

A subsystem acts like a claim store that entails the claims of those subsystems
within its scope.


* composition

Subsystem can be implemented in "branch" and "leaf" ways.

It must be possible to create branch types that act as "components," that is,
systems of other things that can act as though self-contained.

While it must be possible to "chunk" components (treat them as a unit), it must
also be possible to inspect them when they use sub processes.

On the face of it, it would appear that a hard hierarchy of scopes would be too
constraining for many practical purposes.  The difference here is that the
system supports arbitrary lines of communication between nodes.  It is true that
values passed along those lines may cause changes in the process graph elsewhere
in the tree, but such changes are always done by the immediate parent of the
processes (if they choose to do so).

Creating a "component" MUST NOT require creating a subclass.  It must be
possible to do using data only.

* chunking and oblique communication

The fact that communication lines may be set up between any two locations in the
system breaks encapsulation.  We're considering this okay, part of the tradeoff.

More than that, though... I guess is the following channel issue.

* IPC and buffered communication

So far, the discussion of IPC has centered on reactive (event-driven) dataflow.
This allows us to make certain assumptions, particularly that
- message passing is non-blocking
- message receipt is non-destructive to the sender

However, there are situations where this doesn't work.

When a data-processing step is asynchronous, a node cannot continue to receive
messages and still guarantee a consistent output order.  There may also be a
limit on the number of requests that a thing can concurrently handle.

Buffering is the typical way to handle this.

However, buffers have limits.  When inputs are faster than outputs, one option
is to selectively drop values when the buffer is at its configured capacity.

However, this is not always acceptable, either.  In such cases, nodes should be
able to /block writes/.

Combined with the ability of an upstream writer to non-destructively query
whether a channel is able to accept a new value, systems offer a way to signal
backpressure.

These properties (buffering and blocking) have many consequences for 

* invariants

A scope acts as a namespace.

Every scope itself lives in a context.

Each scope may contain zero or more scopes.

Each scope contained by a scope has a name.

Is a scope itself a process?  Yes.

Who decides what change take place in a scope?

The owner.  The scope "itself," which is a process.  That's the whole point.

Don't you want scopes to prevent reference leakage?  Yes, I'm talking about
encapsulation.

There are two kinds of scopes: reified and reflected.

In a reified scope, child processes are driven by facts that are turned into
child processes.  In a reflected system, child processes are not determined as
such but created directly.

How can a reified subsystem be the child of a reflected subsystem, or vice
versa?

* subsystems

A subsystem is a process that can spawn other processes.

A subsystem MUST NOT leak references to the things it creates.

A subsystem MUST hide references to any underlying mechanims (same point as
above).

A subsystem MUST support non-monotonic, declarative, stream-based inter-process
communication:

- declarative :: using data-based descriptions (not concrete mechanisms)

- stream-based :: targeting a specific dataflow (stream & transform) interface

- non-monotonic :: communication channels can be added and removed

PROVISIONAL There are two disjunct types of subsystem:

- reified subsystem :: assertions drive the creating and updating of things
- reflected subsystem :: things drive the claims

** spawn

Each thing spawned by a subsystem must have a unique name within that subsystem.

Does it know its name?
Does it know about its supersystem (containing system)?

* sketch

Interface or implementation?

Implementers would be anything that can't be composed from smaller bits.
Generally they will be wrappers for a built-in or third-party mechanism.

There comes a time in the life of every subsystem when
- it is born
  - with certain fixed, invariant, portable (data-based) properties
- it finds out that a new thing has been created
- it decides to create a new thing
- it finds out that it's time to die
  - e.g. web socket closed
    - but does this mean you're *gone* or just in that state?
- it finds out that one of its children died
  - how does this work for reified systems?
    - can be expected?  considered an error?
    - can/should recreate the thing?
- it is asked (by its parent) to die
- it is asked to set up some IPC?
- the process "itself" has messages to broadcast?
- the process "itself" has ports become available?

* inter-process communication (IPC)

We have talked about inter-process communication (IPC) as a separate layer on
top of the spawn layer, subject to a different set of invariants.

If we think of processes as nodes and communication channels as (the exclusive
form of) edges in a dataflow, then we can say that without IPC, there is no
dataflow.  We can say that, although nodes are facts, and although edges would
also be facts (when they are present), that the dataflow cannot have a /state/
until there are edges to propagate values through it.  Ingresses may be an
exception to this.  An ingress can recieve values without there being an edge
connected to any other node.  In practice, though, most ingresses will be
implemented as streams, in which case a subscription (edge) /would/ be needed to
initiate activity.

What messaging can take place /outside/ of IPC?
- must be sufficient to support creation and destruction of processes

Do parents and children get comunication lines "for free" for that purpose?

The objective to avoid direct object references between systems supports the
notion that communication takes place through channels.

Note that IPC will require references between instances (at least for idiomatic
usage), and this subs/parent navigation is useful to see (when you have a way to
see it).

But won't some API's require direct references between instances?  i.e. there
are some methods that require an instance as an argument?  Outside of DOM, I
can't think of any offhand.

Spawn must be a direct, on-site operation.  (Sort of---even that can be mediated
through system if we have constructable descriptions).




** ports

We can consider that the endpoints of communication channels are associated with
- the process itself
- a (named) port associated with that process

It's possible to devise declarative IPC that targets either or both of those.

* questions

In a reflected system, isn't it possible for a process to die without its child
processes dying?  If so, could you maintain the "contingent" invariant by
reassigning it to an ancestor reified process (assuming the root is always one)?

Isn't it true that
- reify needs to be based around RDF @type
- reflect needs to be based around instance (class or @type)

So you can't really do both in the "same thing".

And why would you want to?

Can this be done through protocol implementations?

* ports, IPC, and facts /versus/ values

Note that the object graph gives you an idea of (some of) the possible lines of
communication in a program.  Where this breaks down is that you don't know what
references are embedded in closures.  Opaque closures really complicate
everything in metaprogramming.  (Private fields will soon add to this
debasement.)

We propose a runtime knowledge base in order to produce software artifacts with
traits that are essential for humans.

The knowledge base is not an end in its own right.  That is, the knowledge base
does not need to be all-knowing.  Some facts belong in the knowledge base
because they are of immediate importance to humans.  Some facts act as mediators
of knowlege: they make it possible to locate other information that does not
need to be in the knowledge base at all times.  Some facts---such as low-level
implementation details---may be knowable in userland but are of no interest.
Such facts we allow to remain as they have always been, subterranean.

We consider a distinction between this first tier (facts belonging in the
knowledge base) and the second tier (facts that are merely reachable).  Arguably
this group includes facts implied by, e.g. ontological or other rules.  With the
help of a backward-chaining engine (which we haven't implemented yet), such
facts could be reachable without being materialized.

But that isn't exactly what I mean here.  Maybe this is the wrong way to state
it.  What I mean is the difference between "the dataflow" and "the data."

Since a model with open semantics can represent any kind of entity, I'm wary of
making "dataflow" first-class from the outset.  Nevertheless, since it is
already possible for /static/ RDF graphs to talk about arbitrary domains in fixed
terms, we may as well at least consider the domain of /dynamic/ entities
(including dynamic graphs themselves) as a specially relevant to the modeling of
runtime activity.  This requires us to adopt at least one fundamental construct
for describing such activity.  There's a good point here, but I'm not quite
hitting it.  Anyway, it's a sideline in this context.

The notion of systems and processes which communicate through message passing is
basic.  Dataflow graphs are a construct that trades arbitrary message passing
for something more organized.  This tradeoff strongly favors the human.  Is it
the /only/ such tradeoff?  Perhaps not, but in any case the difficulty is not with
static dataflow.

If dataflow graphs take us from the unwieldy prospect of arbitrary message
passing into a more tractable space, it's clear that we need to take the next
step to make /arbitrarily changing/ dataflow similarly more tractable.

So it's worth asking how dataflow graphs achieve this win.  Message passing
occurs between identifiable entities (processes, actors, whatever).  Arbitrary
message passing means that any actor can send a message to any other actor at
any time.  Taking the processes as its nodes, a dataflow graph says that
messages may only be passed along its edges, which are usually directed.  In all
non-trivial systems, this greatly constrains how processes may interact.  The
question of what can influence what, for example, can be answered by common
graph analysis tools.  With unconstrained communication, such questions are
unanswerable.  Likewise, clustering algorithms can be used to identify
subsystems, etc.  Not to mention that graphs can be visualized in various ways.
Dataflow graphs "improve on" the general message passing situation by vastly
reducing the space of possibilities, and doing so in a way that is congruent
with a familiar and well-studied construct.

Now for the question of changes.  I downloaded a bunch of papers about dynamic
graph algorithms.  I haven't read them.  Though I'd note that they weren't about
dynamic graph /models/.  I suspect that most of the literature is in the realm of
dynamic graphs that simply report their state, rather than models that describe
how the graph changes.  Rather than stopping to read these papers, in any case,
I'm going to think through the problem as I see it.

Having said that, I should also note that /parts/ of... wait.  I was going to say
that parts of the dataflow graph in a system will be merely reflected rather
than reified.  This is true of nodes (think websocket clients), but I'm not sure
that it's true of edges.  All "wiring" between processes should well be able to
go through the system.

Going back to the point earlier, we can say that the dataflow graph (i.e. lines
of communication), being a set of facts essential to the system.

So yeah I forgot about this.  Over the summer I took the position that the
question of whether a fact belonged in the knowledge base was equivalent to the
question of whether it would need to be included for the purpose of transporting
the model.  This itself begs the question somewhat, because you can define what
is "essential" to the the model in different ways.  Sometimes a model's essence
is independent of its particular state.  But you can imagine cases in which you
can't convey the thing separately from the "transient" state of its processes.

Yet another way to distinguish the dataflow from the data is by rate of change:
we assume that the rate of dataflow is greater than the rate of change in the
dataflow structure itself.  It would seem in these cases that the graph is
failing at its intended purpose, though I haven't put my finger on exactly how.

Somewhat further to the earlier point about the "essence" of the model, we could
say that the dataflow's properties are asserted--- that they are "spoken"
somehow, while the model says nothing about the values.  The space of values is
infinite and generally orthogonal to the communication structure.

But the point here is that the values are not /always/ orthogonal to the
structure.  That is, changes in a dynamic dataflow /must/ be driven by the values
in the dataflow itself (or at least in /some/ dataflow), since the structure
itself is unchanging.

We are talking about a relationship between values and the /next/ state of the
graph.

Moving from open communication to a dataflow graph called for the introduction
of a new concept: lines of communication.

Moving from a graph that changes completely unpredictably to a graph that
changes more predictably must also call for the introuction of a new concept.

Suppose that you view the live (and unconstrained) communication taking place
among a set of actors.  This communication will insinuate a graph.  In practice,
the edges of the graph will be sparse.  That is, there will be pairs of nodes
that simply never communicate directly.  It is essentially the omission of these
potential edges that makes the explicit graph more readable and more telling
than the (dense) graph of /possible/ communication.

So let us consider the space of /possible/ changes to a graph, i.e. all possible
next graphs (next message send, in other example).  Unlike with the
communication channels, we don't have a natural bound on the set of ultimate
possibilities.  In that case, we reach a fix point when all possible edges have
been identified.

But suppose that we only consider the space of possibilities for a single
operation.  In terms of processes (nodes), we can either add a node or remove a
node.  This means that there are 2N possibilities for the next graph where N is
the number of nodes prior to the operation.

